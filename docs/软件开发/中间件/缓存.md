# 缓存

## 缓存的用途

- 高性能

当一个数据获取的请求查询数据库比较耗时（假设是`600ms`），并且查询出来的结果在接下来的几个小时内都不会变化，这时可以将查询的结果放在缓存里，当下次相同的请求发起时，直接从缓存中读取数据，耗时在`2ms`，性能提升300倍。像这种需要复杂操作耗时查出来的结果并且变化不大，但是有很多读请求，直接将查询结果放在缓存中，后续的查询直接查询缓存即可。

- 高并发

MySQL数据库单机大概能支撑`2000QPS`，当系统高峰期的并发量过高时，MySQL单机数据库肯定会挂掉。这时如果将数据放在缓存里，就可以支撑并发量到几万或者几十万。单机承载并发量是MySQL的几十倍。

**缓存是在内存中的，内存天然就支持高并发。**

## 3种常用的缓存读写策略

### Cache Aside Pattern（旁路缓存模式）

`Cache Aside Pattern`是我们平时使用比较多的一种缓存读写模式，比较适合读请求比较多的场景。

`Cache Aside Pattern`中服务端需要同时维系`db`和`cache`，并且是以db的结果为准。

这个模式下的缓存读写步骤如下。

- 写：
  - 先更新`db`
  - 然后直接删除`cache`

  ![CacheAsidePattern写数据](./缓存/CacheAsidePattern写数据.webp)

- 读：
  - 从`cache`中读取数据，读取到就直接返回
  - `cache`中读取不到的话，就从`db`中读取数据返回
  - 再把数据放到`cache`中
  ![CacheAsidePattern读数据](./缓存/CacheAsidePattern读数据.webp)

- 场景一：在写数据的过程中，可以先删除cache，后更新db吗？

肯定是不行的，因为这样可能会造成**数据库（db）和缓存（cache）数据不一致**的问题。

举例：请求1先写数据A，请求2随后读数据A的话，就很有可能产生数据不一致的问题。

这个过程可以简单描述为：

> 请求1先把cache中的A数据删除 -> 请求2从db中读取数据 -> 请求1再把db中的A数据更新

- 场景二：在写数据的过程中，先更新db，后删除cache就没有问题了吗？

理论上来说还是可能会出现数据不一致的问题，不过概率非常小，因为缓存的写入速度是比数据库的写入速度快很多的。

举例：请求1先读数据A，请求2随后写数据A，并且数据A在请求1请求之前不在缓存中的话，也可能产生数据不一致的问题。

这个过程可以简单描述为：

> 请求1从db读数据A -> 请求2更新db中的数据A（此时缓存中无数据A，故不用执行删除缓存的操作）-> 请求1将数据A写入cache

- 缺陷

1. 首次请求数据一定不在cache的问题

    解决办法：可以将热点数据提前放入`cache`中。

2. 写操作比较频繁的话导致`cache`中的数据会频繁被删除，这样会影响缓存命中率

    解决办法：
      - 数据库和缓存数据强一致场景：更新db的时候同样更新cache，不过我们需要加一个锁/分布式锁来保证更新cache的时候不存在线程安全问题。
      - 可以短暂地允许数据库和缓存数据不一致的场景：更新db的时候同样更新cache，但是给缓存加一个比较短的过期时间，这样的话就可以保证即使数据不一致的话影响也比较小。

### Read/Write Through Pattern（读写穿透）

`Read/Write Through Pattern`中服务端把`cache`视为主要的数据存储，从中读取数据并将数据写入其中。`cache`服务负责将此数据读取和写入`db`，从而减轻了应用程序的职责。

这种缓存读写策略在平时开发过程中非常少见。抛去性能方面的影响，大概率是因为我们经常使用的分布式缓存`Redis`并没有提供`cache`将数据写入`db`的功能。

- 写（Write Through）：
  - 先查`cache`，`cache`中不存在，直接更新`db`
  - `cache`中存在，则先更新`cache`，然后`cache`服务自己更新`db`（同步更新`cache`和`db`）
  ![WriteThroughPattern](./缓存/WriteThroughPattern.webp)

- 读（Read Through）：
  - 从`cache`中读取数据，读取到就直接返回
  - 读取不到的话，先从`db`加载，写入到`cache`后返回响应
  ![ReadThroughPattern](./缓存/ReadThroughPattern.webp)

`Read/Write Through Pattern`实际上只是在`Cache Aside Pattern`之上进行了封装。在`Cache Aside Pattern`下，发生读请求的时候，如果`cache`中不存在对应的数据，是由客户端自己负责把数据写入`cache`，而`Read/Write Through Pattern`则是`cache`服务自己来写入缓存的，这对客户端是透明的。

和`Cache Aside Pattern`一样，`Read/Write Through Pattern`也有首次请求数据一定不再`cache`的问题，对于热点数据可以提前放入缓存中。

### Write Behind Pattern（异步缓存写入）

`Write Behind Pattern`和`Read/Write Through Pattern`很相似，两者都是由`cache`服务来负责`cache`和`db`的读写。

但是，两个又有很大的不同：`Read/Write Through Pattern`是同步更新`cache`和`db`，而`Write Behind Pattern`则是只更新缓存，不直接更新`db`，而是改为异步批量的方式来更新`db`。

很明显，这种方式对数据一致性带来了更大的挑战，比如`cache`数据可能还没异步更新`db`的话，`cache`服务可能就挂掉了。

这种策略在我们平时开发过程中也非常非常少见，但不代表它的应用场景少，比如消息队列中消息的异步写入磁盘、MySQL的`Innodb Buffer Pool`机制都用了这种策略。

`Write Behind Pattern`下`db`的写性能非常高，非常适合一些数据经常变化又对数据一致性要求没那么高的场景，比如浏览量、点赞量。

## Redis
### Redis的特性

1. 支持复杂的数据结构

Redis拥有多种数据结构，能支持更丰富的数据操作。如果需要缓存能够支持更复杂的结构和操作，Redis会是不错的选择。

2. 原生支持集群模式

在Redis 3.x版本中，就能支持`cluster`模式。

3. 性能

由于Redis只使用单核，在每一个核上存储小于`100k`的小数据时，性能较高。

### Redis的线程模型

Redis内部使用文件事件处理器`file event handler`，这个文件事件处理器是单线程的，所以Redis才叫做单线程的模型。它采用IO多路复用机制同时监听多个socket，将产生事件的socket压入内存队列中，事件分派器根据socket上的事件类型来选择对应的事件处理器进行处理。

文件事件处理器的结构包含4个部分：

- 多个socket
- IO多路复用工具
- 文件事件分派器
- 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）

多个socket可能会并发产生不同的操作，每个操作对应不同的文件事件，但是IO多路复用程序会监听多个socket，会将产生事件的socket放入队列中排队，事件分派器每次从队列中取出一个socket，根据socket的事件类型交给对应的事件处理器进行处理。

来看客户端与Redis的一次通信过程：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/763022/1674364927062-94e79aaf-78a7-40b4-ba16-eb399e533283.png#averageHue=%236d6d6d&clientId=u92ae7ee9-f065-4&from=paste&id=u3e17950c&originHeight=534&originWidth=924&originalType=url&ratio=1&rotation=0&showTitle=false&size=83073&status=done&style=none&taskId=ud323b27e-00ef-4164-a991-3621e5d912e&title=)

通信是通过socket来完成的。

首先，Redis服务端进程初始化的时候，会将server socket的`AE_READABLE`事件与连接应答处理器关联。

客户端socket01向Redis进程的server socket请求建立连接，此时server socket会产生一个`AE_READAblE`事件，IO多路复用程序监听到server socket产生的事件后，将该socket压入队列中。文件事件分派器从队列中获取socket，交给连接应答处理器。连接应答处理器会创建一个能与客户端通信的socket01，并将改socket01的`AE_READABLE`事件与命令请求处理器关联。

假设此时客户端发送了一个`set key value`请求，此时Redis中的socket01会产生`AE_READABLE`事件，IO多路复用程序将socket01压入队列，此时事件分派器从队列中获取到socket01产生的`AE_READABLE`事件，由于前面socket01的`AE_READABLE`事件已经与命令请求处理器关联，因此事件分派器将事件交给命令请求处理器来处理。命令请求处理器读取socket01的`key value`并在自己内存中完成`key value`的设置。操作完成后，它会将socket01的`AE_WRITABLE`事件与命令回复处理器关联。

如果此时客户端准备好接收返回结果了，那么Redis中的socket01会产生一个`AE_WRITABLE`事件，同样压入队列中，事件分派器找到相关联的命令回复处理器，由命令回复处理器对socket01输入本次操作的一个结果，比如ok，之后解除socket01的`AE_WRITABLE`事件与命令回复处理器的关联。
这样便完成了一次通信。

### Redis单线程模型效率高的原因

- 纯内存操作
- 核心是基于非阻塞的IO多路复用机制
- C语言实现，一般来说，C语言实现的程序“距离”操作系统更近，执行速度相对会更快
- 单线程反而避免了多线程的频繁上下文切换问题，预防了多线程可能产生的竞争问题

### Redis 6.0开始引入多线程

**注意！** Redis 6.0之后的版本抛弃了单线程模型这一设计，原本使用单线程运行的Redis也开始选择性地使用多线程模型。

前面还在强调Redis单线程模型的高效性，现在为什么又要引入多线程？这其实说明Redis在有些方面，单线程已经不具备优势了。因为读写网络的Read/Write系统调用在Redis执行期间占用了大部分CPU时间，如果把对网络读写做成多线程的方式对性能会有很大提升。

**Redis的多线程部分只是用来处理网络数据的读写和协议解析，执行命令仍然是单线程。** 之所以这么设计是不想Redis因为多线程而变得复杂，需要去控制key、lua、事务、LPUSH/LPOP等等的并发问题。

Reids选择使用单线程模型处理客户端的请求主要还是因为CPU不是Redis服务器的瓶颈，所以使用多线程模型带来的性能提升并不能抵消它带来的开发成本和维护成本，系统的性能瓶颈也主要在网络I/O操作上；而Redis引入多线程操作也是出于性能上的考虑，对于一些大键值对的删除操作，通过多线程非阻塞地释放内存空间（释放操作不会阻塞网络IO读写，因为网络IO读写与释放的命令执行不是同一个线程）也能减少对Redis主线程阻塞的时间，提高执行的效率。

### Redis 5种基本数据类型

Redis共有5种基本类型：String（字符串）、List（列表）、Set（集合）、Hash（散列）、Zset（有序集合）。

这5种数据类型是直接提供给用户使用的，是数据的保存形式，其底层实现主要依赖这8种数据结构：简单动态字符串（SDS）、LinkedList（双向链表）、Dict（哈希表/字典）、SkipList（跳跃表）、Intset（整数集合）、ZipList（压缩列表）、QuickList（快速列表）。

Redis 5种基本数据类型对应的底层数据结构实现如下表所示：

| String | List | Hash | Set | Zset |
|---|---|---|---|---|
| SDS | LinkedList/ZipList/QuickList | Dict、ZipList | Dict、Intset | ZipList、SkipList |

Redis 3.2之前，List底层实现是LinkedList或者ZipList。Redis 3.2之后，引入了LinkedList和ZipList的结合QuickList，List的底层实现变为QuickList。从Redis 7.0开始，ZipList被ListPack取代。

可以在Redis官网上找到Redis数据类型/结构非常详细的介绍：

- [Redis Data Structures](https://redis.com/redis-enterprise/data-structures/)
- [Redis Data types tutorial](https://redis.io/docs/manual/data-types/data-types-tutorial/)

#### String（字符串）

String是Redis中最简单同时也是最常用的一个数据类型。

String是一种二进制安全的数据类型，可以用来存储任何类型的数据比如字符串、整数、浮点数、图片（图片的base64编码或者解码或者图片的路径）、序列化后的对象。

![Redis_String](./缓存/Redis_String.png)

虽然Redis是用C语言写的，但是Redis并没有使用C的字符串表示，而是自己构建了一种**简单动态字符串**（Simple Dynamic String，**SDS**）。相比于C的原生字符串，Redis的SDS不光可以保存文本数据还可以保存二进制数据，并且获取字符串长度复杂度为O(1)（C字符串为O(N)），除此之外，Redis的SDS API是安全的，不会造成缓冲区溢出。

##### 常用命令

| 命令 | 介绍 |
|---|---|
| SET key value | 设置指定key的值 |
| SETNX key value | 只有在key不存在时设置key的值 |
| GET key | 获取指定key的值 |
| MSET key1 value1 key2 value2 …… | 设置一个或多个指定key的值 |
| MGET key1 key2 … | 获取一个或多个指定key的值 |
| STRLEN key | 返回key所储存的字符串值的长度 |
| INCR key | 将key中储存的数字值增一 |
| DECR key | 将key中储存的数字值减一 |
| EXISTS key | 判断指定key是否存在 |
| DEL key（通用） | 删除指定的key |
| EXPIRE key seconds（通用） | 给指定key设置过期时间 |

更多Redis String命令以及详细使用指南，请查看Redis官网对应的介绍：[https://redis.io/commands/?group=string](https://redis.io/commands/?group=string)

**基本操作：**

```bash
> SET key value
OK
> GET key
"value"
> EXISTS key
(integer) 1
> STRLEN key
(integer) 5
> DEL key
(integer) 1
> GET key
(nil)
```

**批量设置：**

```bash
> MSET key1 value1 key2 value2
OK
> MGET key1 key2 # 批量获取多个 key 对应的 value
1) "value1"
2) "value2"
```

**计数器（字符串的内容为整数的时候可以使用）：**

```bash
> SET number 1
OK
> INCR number # 将 key 中储存的数字值增一
(integer) 2
> GET number
"2"
> DECR number # 将 key 中储存的数字值减一
(integer) 1
> GET number
"1"
```

**设置过期时间（默认为永不过期）：**

```bash
> EXPIRE key 60
(integer) 1
> SETEX key 60 value # 设置值并设置过期时间
OK
> TTL key
(integer) 56
```

##### 应用场景

- 需要存储常规数据的场景

  - 举例：缓存Session、Token、图片地址、序列化后的对象（相比较于Hash存储更节省内存）。
  - 相关命令：`SET`、`GET`。

- 需要计数的场景

  - 举例：用户单位时间的请求数（简单限流可以用到）、页面单位时间的访问数。
  - 相关命令：`SET`、`GET`、`INCR`、`DECR`。

- 分布式锁

  利用`SETNX key value`命令可以实现一个简易的分布式锁（存在一些缺陷，通常不建议这样实现分布式锁）。

#### List（列表）

Redis中的List其实就是链表数据结构的实现。

许多高级编程语言都内置了链表的实现，比如Java中的`LinkedList`，但是C语言并没有实现链表，所以Redis实现了自己的链表数据结构。Redis的List的实现为一个**双向链表**，既可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销。

![Redis_List](./缓存/Redis_List.png)

##### 常用命令

|命令|介绍|
|---|---|
| RPUSH key value1 value2 … | 在指定列表的尾部（右边）添加一个或多个元素 |
| LPUSH key value1 value2 … | 在指定列表的头部（左边）添加一个或多个元素 |
| LSET key index value | 将指定列表索引index位置的值设置为value |
| LPOP key | 移除并获取指定列表的第一个元素（最左边） |
| RPOP key | 移除并获取指定列表的最后一个元素（最右边） |
| LLEN key | 获取列表元素数量 |
| LRANGE key start end | 获取列表start和end之间的元素 |

更多Redis List命令以及详细使用指南，请查看Redis官网对应的介绍：[https://redis.io/commands/?group=list](https://redis.io/commands/?group=list)

**通过`RPUSH/LPOP`或者`LPUSH/RPOP`实现队列：**

```bash
> RPUSH myList value1
(integer) 1
> RPUSH myList value2 value3
(integer) 3
> LPOP myList
"value1"
> LRANGE myList 0 1
1) "value2"
2) "value3"
> LRANGE myList 0 -1
1) "value2"
2) "value3"
```

**通过`RPUSH/RPOP`或者`LPUSH/LPOP`实现栈：**

```bash
> RPUSH myList2 value1 value2 value3
(integer) 3
> RPOP myList2 # 将 list的最右边的元素取出
"value3"
```

以下是`RPUSH、LPOP、LPUSH、RPOP命令的示意图：

![Redis_List_command](./缓存/Redis_List_command.png)

**通过`LRANGE`查看对应下标范围的列表元素：**

```bash
> RPUSH myList value1 value2 value3
(integer) 3
> LRANGE myList 0 1
1) "value1"
2) "value2"
> LRANGE myList 0 -1
1) "value1"
2) "value2"
3) "value3"
```

通过`LRANGE`命令，可以基于List实现分页查询，性能非常高！

**通过`LLEN`查看链表长度：**

```bash
> LLEN myList
(integer) 3
```

##### 应用场景

- 信息流展示

  - 举例：最新文章、最新动态。
  - 相关命令：`LPUSH`、`LRANGE`。

- 消息队列

  `List`可以用来做消息队列，只是功能过于简单且存在很多缺陷，不建议这样做。

  相对来说，Redis 5.0新增加的一个数据结构`Stream`更适合做消息队列，只是功能依然非常简陋。和专业的消息队列相比，还是有甚多欠缺的地方比如消息丢失和堆积问题不好解决。

#### Hash（哈希）

Redis中的Hash是一个String类型的field-value（键值对）映射表，特别适合用于存储对象，后续操作的时候，你可以直接修改这个对象中的某些字段的值。

Hash类似于JDK 1.8前的`HashMap`，内部实现也差不多（数组 + 链表）。不过，Redis的Hash做了更多优化。

![Redis_Hash](./缓存/Redis_Hash.png)

##### 常用命令

|命令|介绍|
|---|---|
| HSET key field value | 设置指定哈希表中指定字段的值 |
| HSETNX key field value | 只有指定字段不存在时设置指定字段的值 |
| HMSET key field1 value1 field2 value2 …… | 同时将一个或多个field-value（域-值）对设置到指定哈希表中 |
| HGET key field | 获取指定哈希表中指定字段的值 |
| HMGET key field1 field2 … | 获取指定哈希表中一个或者多个指定字段的值 |
| HGETALL key | 获取指定哈希表中所有的键值对 |
| HEXISTS key field | 查看指定哈希表中指定的字段是否存在 |
| HDEL key field1 field2 … | 删除一个或多个哈希表字段 |
| HLEN key | 获取指定哈希表中字段的数量 |
| HINCRBY key field increment | 对指定哈希中的指定字段做运算操作（正数为加，负数为减） |

更多Redis Hash命令以及详细使用指南，请查看Redis官网对应的介绍：[https://redis.io/commands/?group=hash](https://redis.io/commands/?group=hash)

**模拟对象数据存储：**

```bash
> HMSET userInfoKey name "guide" description "dev" age 24
OK
> HEXISTS userInfoKey name # 查看 key 对应的 value中指定的字段是否存在。
(integer) 1
> HGET userInfoKey name # 获取存储在哈希表中指定字段的值。
"guide"
> HGET userInfoKey age
"24"
> HGETALL userInfoKey # 获取在哈希表中指定 key 的所有字段和值
1) "name"
2) "guide"
3) "description"
4) "dev"
5) "age"
6) "24"
> HSET userInfoKey name "GuideGeGe"
> HGET userInfoKey name
"GuideGeGe"
> HINCRBY userInfoKey age 2
(integer) 26
```

##### 应用场景

- 对象数据存储场景

  - 举例：用户信息、商品信息、文章信息、购物车信息。
  - 相关命令：`HSET`（设置单个字段的值）、`HMSET`（设置多个字段的值）、`HGET`（获取单个字段的值）、`HMGET`（获取多个字段的值）。

#### Set（集合）

Redis中的Set类型是一种无序集合，集合中的元素没有先后顺序但都唯一，有点类似于Java中的`HashSet`。当你需要存储一个列表数据，又不希望出现重复数据时，Set是一个很好的选择，并且Set提供了判断某个元素是否在一个Set集合内的重要接口，这个也是List所不能提供的。

你可以基于Set轻易实现交集、并集、差集的操作，比如你可以将一个用户所有关注人存在一个集合中，将其所有粉丝存在一个集合。这样的话，Set可以非常方便的实现如共同关注、共同粉丝、共同喜好等功能。这个过程也就是求交集的过程。

![Redis_Set](./缓存/Redis_Set.png)

##### 常用命令

|命令|介绍|
|---|---|
| SADD key member1 member2 … | 向指定集合添加一个或多个元素 |
| SMEMBERS key | 获取指定集合中的所有元素 |
| SCARD key | 获取指定集合的元素数量 |
| SISMEMBER key member | 判断指定元素是否在指定集合中 |
| SINTER key1 key2 … | 获取给定所有集合的交集 |
| SINTERSTORE destination key1 key2 … | 将给定所有集合的交集存储在destination中 |
| SUNION key1 key2 … | 获取给定所有集合的并集 |
| SUNIONSTORE destination key1 key2 … | 将给定所有集合的并集存储在destination中 |
| SDIFF key1 key2 … | 获取给定所有集合的差集 |
| SDIFFSTORE destination key1 key2 … | 获取给定所有集合的差集存储在destination中 |
| SPOP key count | 随机移除并获取指定集合中一个或多个元素 |
| SRANDMEMBER key count | 随机获取指定集合中指定数量的元素 |

更多Redis Set命令以及详细使用指南，请查看Redis官网对应的介绍：[https://redis.io/commands/?group=set](https://redis.io/commands/?group=set)。

**基本操作：**

```bash
> SADD mySet value1 value2
(integer) 2
> SADD mySet value1 # 不允许有重复元素，因此添加失败
(integer) 0
> SMEMBERS mySet
1) "value1"
2) "value2"
> SCARD mySet
(integer) 2
> SISMEMBER mySet value1
(integer) 1
> SADD mySet2 value2 value3
(integer) 2
```

- `mySet`: `value1`、`value2`。
- `mySet2`: `value2`、`value3`。

**求交集：**

```bash
> SINTERSTORE mySet3 mySet mySet2
(integer) 1
> SMEMBERS mySet3
1) "value2"
```

**求并集：**

```bash
> SUNION mySet mySet2
1) "value3"
2) "value2"
3) "value1"
```

**求差集：**

```bash
> SDIFF mySet mySet2 # 差集是由所有属于 mySet 但不属于 A 的元素组成的集合
1) "value1"
```

##### 应用场景

- 需要存放的数据不能重复的场景

  - 举例：网站UV统计（数据量巨大的场景还是`HyperLogLog`更适合一些）、文章点赞、动态点赞等场景。
  - 相关命令：`SCARD`（获取集合数量）。

- 需要获取多个数据源交集、并集和差集的场景

  - 举例：共同好友（交集）、共同粉丝（交集）、共同关注（交集）、好友推荐（差集）、音乐推荐（差集）、订阅号推荐（差集 + 交集）等场景。
  - 相关命令：`SINTER`（交集）、`SINTERSTORE`（交集）、`SUNION`（并集）、`SUNIONSTORE`（并集）、`SDIFF`（差集）、`SDIFFSTORE`（差集）。

- 需要随机获取数据源中的元素的场景

  - 举例：抽奖系统、随机点名等场景。
  - 相关命令：`SPOP`（随机获取集合中的元素并移除，适合不允许重复中奖的场景）、`SRANDMEMBER`（随机获取集合中的元素，适合允许重复中奖的场景）。

#### Sorted Set（有序集合）

Sorted Set类似于Set，但和Set相比，Sorted Set增加了一个权重参数`score`，使得集合中的元素能够按`score`进行有序排列，还可以通过`score`的范围来获取元素的列表。有点像是Java中`HashMap`和`TreeSet`的结合体。

![Redis_SortedSet](./缓存/Redis_SortedSet.png)

##### 常用命令

|命令|介绍|
|---|---|
| ZADD key score1 member1 score2 member2 …… | 向指定有序集合添加一个或多个元素 |
| ZCARD KEY | 获取指定有序集合的元素数量 |
| ZSCORE key member | 获取指定有序集合中指定元素的score值 |
| ZINTERSTORE destination numkeys key1 key2 … | 将给定所有有序集合的交集存储在destination中，对相同元素对应的score值进行SUM聚合操作，numkeys为集合数量 |
| ZUNIONSTORE destination numkeys key1 key2 … | 求并集，其它和ZINTERSTORE类似 |
| ZDIFFSTORE destination numkeys key1 key2 … | 求差集，其它和ZINTERSTORE类似 |
| ZRANGE key start end | 获取指定有序集合start和end之间的元素（score从低到高） |
| ZREVRANGE key start end | 获取指定有序集合start和end之间的元素（score从低到高） |
| ZREVRANK key member | 获取指定有序集合中指定元素的排名（score从大到小排序） |

更多Redis Sorted Set命令以及详细使用指南，请查看Redis官网对应的介绍：[https://redis.io/commands/?group=sorted-set](https://redis.io/commands/?group=sorted-set)。

**基本操作：**

```bash
> ZADD myZset 2.0 value1 1.0 value2
(integer) 2
> ZCARD myZset
2
> ZSCORE myZset value1
2.0
> ZRANGE myZset 0 1
1) "value2"
2) "value1"
> ZREVRANGE myZset 0 1
1) "value1"
2) "value2"
> ZADD myZset2 4.0 value2 3.0 value3
(integer) 2
```

- `myZset`: `value1`(2.0)、`value2`(1.0)。
- `myZset2`: `value2`(4.0)、`value3`(3.0)。

**获取指定元素的排名：**

```bash
> ZREVRANK myZset value1
0
> ZREVRANK myZset value2
1
```

**求交集：**

```bash
> ZINTERSTORE myZset3 2 myZset myZset2
1
> ZRANGE myZset3 0 1 WITHSCORES
value2
5
```

**求并集：**

```bash
> ZUNIONSTORE myZset4 2 myZset myZset2
3
> ZRANGE myZset4 0 2 WITHSCORES
value1
2
value3
3
value2
5
```

**求差集：**

```bash
> ZDIFF 2 myZset myZset2 WITHSCORES
value1
2
```

##### 应用场景

- 需要随机获取数据源中的元素根据某个权重进行排序的场景

  - 举例：各种排行榜比如直播间送礼物的排行榜、朋友圈的微信步数排行榜、王者荣耀中的段位排行榜、话题热度排行榜等等。
  - 相关命令：`ZRANGE`（从小到大排列）、`ZREVRANGE`（从大到小排列）、`ZREVRANK`（指定元素排名）。

- 需要存储的数据有优先级或者重要程度的场景

  - 举例：优先级任务队列。
  - 相关命令：`ZRANGE`（从小到大排列）、`ZREVRANGE`（从大到小排列）、`ZREVRANK`（指定元素排名）。

#### 总结

| 数据类型 | 说明 |
|---|---|
| String | 一种二进制安全的数据类型，可以用来存储任何类型的数据比如字符串、整数、浮点数、图片（图片的base64编码或者解码或者图片的路径）、序列化后的对象。 |
| List | Redis的List的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销。 |
| Hash | 一个String类型的field-value（键值对）的映射表，特别适合用于存储对象，后续操作的时候，你可以直接修改这个对象中的某些字段的值。 |
| Set | 无序集合，集合中的元素没有先后顺序但都唯一，有点类似于Java中的`HashSet`。 |
| Zset | 和Set相比，Sorted Set增加了一个权重参数`score`，使得集合中的元素能够按`score`进行有序排列，还可以通过`score`的范围来获取元素的列表。有点像是Java中`HashMap`和`TreeSet`的结合体。 |

### Redis 3种特殊数据类型

除了5中基本的数据类型之外，Redis还支持3种特殊的数据类型：Bitmap、HyperLogLog、GEO。

#### Bitmap（位图）

根据官网介绍：

> Bitmaps are not an actual data type, but a set of bit-oriented operations defined on the String type which is treated like a bit vector. Since strings are binary safe blobs and their maximum length is 512 MB, they are suitable to set up to 2^32 different bits. <br> Bitmap 不是 Redis 中的实际数据类型，而是在 String 类型上定义的一组面向位的操作，将其视为位向量。由于字符串是二进制安全的块，且最大长度为 512 MB，它们适合用于设置最多 2^32 个不同的位。

Bitmap存储的是连续的二进制数字（0和1），通过Bitmap，只需要一个bit位来表示某个元素对应的值或者状态，key就是对应元素本身。我们知道8个bit可以组成一个byte，所以Bitmap本身会极大的节省储存空间。

你可以将Bitmap看作是一个存储二进制数字（0和1）的数组，数组中每个元素的下标叫做offset（偏移量）。

![Redis_Bitmap](./缓存/Redis_Bitmap.png)

##### 常用命令

|命令|说明|
|---|---|
| SETBIT key offset value | 设置指定offset位置的值 |
| GETBIT key offset | 获取指定offset位置的值 |
| BITCOUNT key start end | 获取start到end之间值为1的元素个数 |
| BITOP operation destkey key1 key2 … | 对一个或多个Bitmap进行运算，可用运算符有AND，OR，XOR以及NOT |

**Bitmap基本操作演示：**

```bash
# SETBIT 会返回之前位的值（默认是 0）这里会生成 7 个位
> SETBIT mykey 7 1
(integer) 0
> SETBIT mykey 7 0
(integer) 1
> GETBIT mykey 7
(integer) 0
> SETBIT mykey 6 1
(integer) 0
> SETBIT mykey 8 1
(integer) 0
# 通过 bitcount 统计被被设置为 1 的位的数量。
> BITCOUNT mykey
(integer) 2
```

##### 应用场景

- 需要保存状态信息（0/1即可表示）的场景

  - 举例：用户签到情况、活跃用户情况、用户行为统计（比如是否点赞过某个视频）。
  - 相关命令：`SETBIT`、`GETBIT`、`BITCOUNT`、`BITOP`。

#### HyperLogLog（基数统计）

HyperLogLog是一种有名的基数计数概率算法，基于LogLog Counting（LLC）优化改进得来，并不是Redis特有的，Redis只是实现了这个算法并提供了一些开箱即用的API。

Redis提供的HyperLogLog占用空间非常小，只需要12k的空间就能存储接近`2^64`个不同元素。并且，Redis对HyperLogLog的存储结构做了优化，采用两种方式计数：

- **稀疏矩阵**：计数较少的时候，占用空间很小。
- **稠密矩阵**：计数达到某个阈值的时候，占用12k的空间。

基数计数概率算法为了节省内存并不会直接存储元数据，而是通过一定的概率统计方法预估基数值（集合中包含元素的个数）。因此，HyperLogLog的计数结果并不是一个精确值，存在一定的误差（标准误差为0.81%）。

![Redis_HyperLogLog](./缓存/Redis_HyperLogLog.png)

HyperLogLog的使用非常简单，但原理非常复杂。HyperLogLog的原理以及在Redis中的实现可以看这篇文章：[HyperLogLog 算法的原理讲解以及 Redis 是如何应用它的](https://juejin.cn/post/6844903785744056333)。

除了HyperLogLog之外，Redis还提供了其他的概率数据结构，对应的官方文档地址：[https://redis.io/docs/data-types/probabilistic/](https://redis.io/docs/data-types/probabilistic/)。

##### 常用命令

HyperLogLog相关的命令非常少，最常用的也就3个。

|命令|说明|
|---|---|
| PFADD key element1 element2 … | 添加一个或多个元素到HyperLogLog中 |
| PFCOUNT key1 key2 | 获取一个或者多个HyperLogLog的唯一计数 |
| PFMERGE destkey sourcekey1 sourcekey2 … | 将多个HyperLogLog合并到destkey中，destkey会结合多个源，算出对应的唯一计数 |

**HyperLogLog基本操作演示：**

```bash
> PFADD hll foo bar zap
(integer) 1
> PFADD hll zap zap zap
(integer) 0
> PFADD hll foo bar
(integer) 0
> PFCOUNT hll
(integer) 3
> PFADD some-other-hll 1 2 3
(integer) 1
> PFCOUNT hll some-other-hll
(integer) 6
> PFMERGE desthll hll some-other-hll
"OK"
> PFCOUNT desthll
(integer) 6
```

##### 应用场景

- 数据量巨大（百万、千万级别以上）的计数场景

  - 举例：热门网站每日/每周/每月访问ip数统计、热门帖子uv统计。
  - 相关命令：`PFADD`、`PFCOUNT`。

#### Geospatial（地理位置）

Geospatial index（地理空间索引，简称GEO）主要用于存储地理位置信息，基于Sorted Set实现。通过GEO我们可以轻松实现两个位置距离的计算、获取指定位置附近的元素等功能。

![Redis_GEO](./缓存/Redis_GEO.png)

##### 常用命令

|命令|说明|
|---|---|
| GEOADD key longitude1 latitude1 member1 … | 添加一个或多个元素对应的经纬度信息到GEO中 |
| GEOPOS key member1 member2 … | 返回给定元素的经纬度信息 |
| GEODIST key member1 member2 M/KM/FT/MI | 返回两个给定元素之间的距离 |
| GEORADIUS key longitude latitude radius distance | 获取指定位置附近distance范围内的其他元素，支持ASC（由近到远）、DESC（由远到近）、Count（数量）等参数 |
| GEORADIUSBYMEMBER key member radius distance | 类似于GEORADIUS命令，只是参照的中心点是GEO中的元素 |

**基本操作：**

```bash
> GEOADD personLocation 116.33 39.89 user1 116.34 39.90 user2 116.35 39.88 user3
3
> GEOPOS personLocation user1
116.3299986720085144
39.89000061669732844
> GEODIST personLocation user1 user2 km
1.4018
```

通过Redis可视化工具查看`personLocation`，可以看到底层就是Sorted Set。

GEO中存储的地理位置信息的经纬度数据通过GeoHash算法转换成了一个整数，这个整数作为Sorted Set的score（权重参数）使用。

![Redis_GeoHash](./缓存/Redis_GeoHash.png)

**获取指定位置范围内的其他元素：**

```bash
> GEORADIUS personLocation 116.33 39.87 3 km
user3
user1
> GEORADIUS personLocation 116.33 39.87 2 km
> GEORADIUS personLocation 116.33 39.87 5 km
user3
user1
user2
> GEORADIUSBYMEMBER personLocation user1 5 km
user3
user1
user2
> GEORADIUSBYMEMBER personLocation user1 2 km
user1
user2
```

`GEORADIUS`命令的底层原理解析可以看看阿里的这篇文章：[Redis 到底是怎么实现“附近的人”这个功能的呢？](https://juejin.cn/post/6844903966061363207)。

**移除元素：**

GEO底层是Sorted Set，你可以对GEO使用Sorted Set相关的命令。

```bash
> ZREM personLocation user1
1
> ZRANGE personLocation 0 -1
user3
user2
> ZSCORE personLocation user2
4069879562983946
```

##### 应用场景

- 需要管理使用地理空间数据的场景

  - 举例：附近的人。
  - 相关命令：`GEOADD`、`GEORADIUS`、`GEORADIUSBYMEMBER`。

#### 总结

|数据类型|说明|
|---|---|
| Bitmap | 你可以将 Bitmap 看作是一个存储二进制数字（0 和 1）的数组，数组中每个元素的下标叫做 offset（偏移量）。通过 Bitmap, 只需要一个 bit 位来表示某个元素对应的值或者状态，key 就是对应元素本身 。我们知道 8 个 bit 可以组成一个 byte，所以 Bitmap 本身会极大的节省储存空间。 |
| HyperLogLog | Redis 提供的 HyperLogLog 占用空间非常非常小，只需要 12k 的空间就能存储接近2^64个不同元素。不过，HyperLogLog 的计数结果并不是一个精确值，存在一定的误差（标准误差为 0.81% ）。 |
| Geospatial index | Geospatial index（地理空间索引，简称 GEO） 主要用于存储地理位置信息，基于 Sorted Set 实现。 |

### Redis持久化机制

使用缓存的额时候，我们经常需要对内存中的数据进行持久化也就是将内存中的数据写到硬盘中。大部分原因是为了之后重用数据（比如重启机器、机器故障之后恢复数据），或者是为了做数据同步（比如Redis集群的主从节点通过RDB文件同步数据）。

Redis不同于Memcached的很重要一点就是，Redis支持持久化，而且支持3种持久化方式：
- 快照（snapshotting，RDB）
- 只追加文件（append-only file，AOF）
- RDB和AOF的混合持久化（Redis4.0 新增）

官方文档地址：[https://redis.io/topics/persistence](https://redis.io/topics/persistence)

![Redis_persitence](./缓存/Redis_persitence.png)

#### RDB持久化

##### 什么是RDB持久化？

Redis可以通过创建快照来获得存储在内存里面的数据在**某个时间点**上的副本。Redis创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis主从结构，主要用来提高Redis性能），还可以将快照留在原地以便重启服务器的时候使用。

快照持久化是Redis默认采用的持久化方式，在`redis.conf`配置文件中默认有以下配置：

```
save 900 1           #在900秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发bgsave命令创建快照。

save 300 10          #在300秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发bgsave命令创建快照。

save 60 10000        #在60秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发bgsave命令创建快照。
```

##### RDB创建快照时会阻塞主线程吗？

Redis提供了两个命令来生成RDB快照文件：

- `save`：同步保存操作，会阻塞Redis主线程；
- `bgsave`：fork出一个子进程，子进程执行，不会阻塞Redis主线程，默认选项。

> 这里说Redis主线程而不是主进程，主要是因为Redis启动后抓哟是通过单线程的方式完成主要的工作。如果你想将其描述为Redis主进程，也没问题。

#### AOF持久化

##### 什么是AOF持久化？

与快照持久化相比，AOF持久化的实时性更好。默认情况下Redis没有开启AOF（append only file）方式的持久化（Redis 6.0之后已经默认是开启了），可以通过appendonly参数开启：

```
appendonly yes
```

开启AOF持久化后每执行一条会更改Redis中数据的命令，Redis就会将该命令写入到AOF缓冲区`saver.aof_buf`中，然后再写入到AOF文件中（此时还在系统内核缓存区未同步到磁盘），最后再根据持久化方式（`fsync`策略）的配置来决定何时将系统内核缓存区的数据同步到硬盘中的。

只有同步到磁盘中才算持久化保存了，否则依然存在数据丢失的风险，比如说：系统内核缓存区的数据还未同步，磁盘机器就宕机了，那这部分数据就算丢失了。

AOF文件的保存位置和RDB文件的位置相同，都是通过`dir`参数设置的，默认的文件名是`appendonly.aof`。

##### AOF工作基本流程是怎样的？

AOF持久化功能的实现可以简单分为5步：

1. **命令追加（append）：** 所有的写命令会追加到AOF缓冲区中。
2. **文件写入（write）：** 将AOF缓冲区的数据写入AOF文件中。这一步需要调用`write`函数（系统调用），`write`将数据写入到了系统内核缓冲区之后直接返回了（延迟写）。注意：此时并没有同步到磁盘。
3. **文件同步（fsync）：** AOF缓冲区根据对应的持久化方式（`fsync`策略）向硬盘做同步操作。这一步需要调用`fsync`函数（系统调用），`fsync`针对单个文件操作，对其进行强制硬盘同步，`fsync`将阻塞直到写入磁盘完成后返回，保证了数据持久化。
4. **文件重写（rewrite）：** 随着AOF文件越来越大，需要定期对AOF文件进行重写，达到压缩的目的。
5. **重启加载（load）：** 当Redis重启时，可以加载AOF文件进行数据恢复。

> Linux系统直接提供了一些函数用于对文件和设备进行访问和控制，这些函数被称为**系统调用（syscall）**。

这里对上面提到的一些Linux系统调用再做一遍解释：

- `write`：写入系统内核缓冲区之后直接返回（仅仅是写到缓冲区），不会立即同步到硬盘。虽然提高了效率，但也带来了数据丢失的风险。同步硬盘操作通常依赖于系统调度机制，Linux内核通常为30s同步一次，具体值取决于写出的数据量和I/O缓冲区的状态。
- `fsync`：`fsync`用于强制刷新系统内核缓冲区（同步到磁盘），确保写磁盘操作结束才会返回。

AOF工作流程图如下：

![AOF工作基本流程](./缓存/AOF工作基本流程.webp)

##### AOF持久化方式有哪些？

在Redis的配置文件中存在三种不同的AOF持久化方式（`fsync`策略），它们分别是：

1. `appendfsync always`：主线程调用`write`执行写操作后，后台线程（`aof_fsync`线程）立即会调用`fsync`函数同步AOF文件（刷盘），`fsync`完成后线程返回，这样会严重降低Redis的性能（`write` + `fsync`）。

2. `appendfsync everysec`：主线程调用`write`执行写操作后立即返回，由后台线程（`aof_fsync`线程）每秒钟调用`fsync`函数（系统调用）同步一次AOF文件（`write` + `fsync`，`fsync`间隔为1秒）。

3. `appendfsync no`：主线程调用`write`执行写操作后立即返回，让操作系统决定何时进行同步，Linux下一班为30秒一次（`write`但不`fsync`，`fsync`的时机由操作系统决定）。

可以看出：**这3种持久化方式的主要区别在于fsync同步AOF文件的额时机（刷盘）**。

为了兼顾数据和写入性能，可以考虑`appendfsync everysec`选项，让Redis每秒同步一次AOF文件，Redis性能受到的影响较小。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。

从Redis 7.0.0开始，Redis使用了**Multi Part AOF**机制。顾名思义，Multi Part AOF就是将原来的单个AOF文件拆分成多个AOF文件。在Multi Part AOF中，AOF文件被分为三种类型，分别为：

- BASE：表示基础AOF文件，它一般由紫禁城通过重写产生，该文件最多只有一个。
- INCR：表示增量AOF文件，它一般会在AOFRW开始执行时被创建，该文件可能存在多个。
- HISTORY：表示历史AOF文件，它由BASE和INCR AOF变化而来，每次AOFRW成功完成时，本次AOFRW之前对应的BASE和INCR AOF都将变为HISTORY，HISTORY类型的AOF会被Redis自动删除。

Multi Part AOF不是重点，了解即可，详细介绍可以看看：[Redis 7.0 Multi Part AOF 的设计和实现](https://zhuanlan.zhihu.com/p/467217082)。

##### AOF为什么是在执行完命令之后记录日志？

关系型数据库（如MySQL）通常都是执行命令之前记录日志（方便故障恢复），而Redis AOF持久化机制是在执行完命令之后再记录日志。

![AOF记录日志过程](./缓存/AOF记录日志过程.webp)

**为什么是在执行完命令之后记录日志呢？**

- 避免额外的检查开销，AOF记录日志不会对命令进行语法检查；
- 在命令执行完之后再记录，不会阻塞当前的命令执行。

这样也带来了风险（上文的AOF持久化中也提到过）：

- 如果刚执行完命令Redis就宕机回导致对应的修改丢失；
- 可能会阻塞后续其他命令的执行（AOF记录日志是在Redis主线程中进行的）。

##### AOF重写

当AOF变得太大时，Redis能够在后台自动重写AOF产生一个新的AOF文件，这个新的AOF文件和原有的AOF文件所保存的数据库状态一样，但体积更小。

![AOF重写](./缓存/AOF重写.webp)

> AOF重写（rewrite）是一个有歧义的名称，该功能是通过读取数据库中的键值对来实现的，程序无须对现有AOF文件进行任务读入、分析或者写入操作。

由于AOF重写会进行大量的写入操作，为了避免对Redis正常处理命令请求造成影响，Redis将AOF重写程序放到子进程里执行。

AOF文件重写期间，Redis还会维护一个**AOF重写缓冲区**，该缓冲区会在子进程创建新AOF文件期间，记录服务器执行的所有写命令。当子进程完成创建新AOF文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新AOF文件的末尾，使得新的AOF文件保存的数据库状态于现有的数据库状态一致。最后，服务器用新的AOF文件替换旧的AOF文件，以此来完成AOF文件重写操作。

开启AOF重写功能，可以调用`BGREWRITEAOF`命令手动执行，也可以设置以下两个配置项，让程序自动决定触发时机：

- `auto-aof-rewrite-min-size`：如果AOF文件大小小于该值，则不会触发AOF重写。默认值为64MB；

- `auto-aof-rewrite-percentage`：执行AOF重写时，当前AOF大小（aof_current_size）和上一次重写时AOF大小（aof_base_size）的比值。如果当前AOF文件大小增加了这个百分比值，将触发AOF重写。将此值设置为0将禁用自动AOF重写。默认值为100。

Redis 7.0版本之前，如果在重写期间有写入命令，AOF可能会使用大量内存，重写期间到达的所有写入命令都会写入磁盘两次。

Redis 7.0版本之后，AOF重写机制得到了优化改进。以下是相关介绍，来自[从 Redis7.0 发布看 Redis 的过去与未来](https://mp.weixin.qq.com/s/RnoPPL7jiFSKkx3G4p57Pg)这篇文章。

> AOF 重写期间的增量数据如何处理一直是个问题，在过去写期间的增量数据需要在内存中保留，写结束后再把这部分增量数据写入新的 AOF 文件中以保证数据完整性。可以看出来 AOF 写会额外消耗内存和磁盘 IO，这也是 Redis AOF 写的痛点，虽然之前也进行过多次改进但是资源消耗的本质问题一直没有解决。阿里云的 Redis 企业版在最初也遇到了这个问题，在内部经过多次迭代开发，实现了 Multi-part AOF 机制来解决，同时也贡献给了社区并随此次 7.0 发布。具体方法是采用 base（全量数据）+inc（增量数据）独立文件存储的方式，彻底解决内存和 IO 资源的浪费，同时也支持对历史 AOF 文件的保存管理，结合 AOF 文件中的时间信息还可以实现 PITR 按时间点恢复（阿里云企业版 Tair 已支持），这进一步增强了 Redis 的数据可靠性，满足用户数据回档等需求。

##### AOF校验机制

AOF校验机制是Redis在启动时对AOF进行检查，以判断文件是否完整，是否有损坏或者丢失的数据。这个机制的原理其实非常简单，就是通过使用一种叫做 **校验和（checksum）** 的数字来验证AOF文件。这个校验和是通过对整个AOF文件内容进行CRC64算法计算得出的数字。如果文件内容发生了变化，那么校验和也会随之改变。因此，Redis在启动时会比较计算出的校验和与文件末尾保存的校验和（计算的时候会把最后一行保存校验和的内容给忽略掉），从而判断AOF文件是否完整。如果发现文件有问题，Redis就会拒绝启动并提供相应的错误信息。AOF校验机制十分简单有效，可以提高Redis数据的可靠性。

类似地，RDB文件也有类似的校验机制来保证RDB文件的正确性，这里就不重复进行介绍了。

#### Redis 4.0对于持久化机制做了什么优化？

由于RDB和AOF各有优势，于是，Redis 4.0开始支持RDB和AOF的混合持久化（默认关闭，可以通过配置项`aof-use-rdb-preamble`开启）。

如果把混合持久化打开，AOF重写的时候就直接把RDB的内容写到AOF文件开头。这样做的好处是可以结合RDB和AOF的优点，快速加载同时避免丢失过多的数据。当然缺点也是有的，AOF里面的RDB部分是压缩格式不再是AOF格式，可读性较差。

官方文档地址：[https://redis.io/topics/persistence](https://redis.io/topics/persistence)

![Redis_persitence](./缓存/Redis_persitence.png)

#### 如何选择RDB和AOF？

关于RDB和AOF的优缺点，官网上面也给了比较详细的说明，这里结合个人理解简单总结一下。

**RDB比AOF优秀的地方：**

- RDB文件存储的内容是经过压缩的二进制数据，保存着某个时间点的数据集，文件很小，适合做数据的备份，灾难恢复。AOF文件存储的是每一次写命令，类似于MySQL的binlog日志，通常会比RDB文件大很多。当AOF变得太大时，Redis能够在后台自动重写AOF。新的AOF文件和原有的AOF文件所保存的额数据库状态一样，但体积更小。不过，Redis 7.0版本之前，如果在重写期间有写入命令，AOF可能会使用大量内存，重写期间到达的所有写入命令都会写入磁盘两次。
- 使用RDB文件恢复数据，直接解析解析还原数据即可，不需要一条一条地执行命令，速度非常快。而AOF则需要以此执行每个写命令，速度非常慢。也就是说，与AOF相比，恢复大数据集的时候，RDB速度更快。

**AOF比RDB优秀的地方：**

- RDB的数据安全性不如AOF，没有办法实时或者秒级持久化数据。生成RDB文件的过程是比较繁重的，虽然BGSAVE子进程写入RDB文件的工作不会阻塞主线程，但会对机器的CPU资源和内存资源产生影响，严重的情况下甚至会直接把Redis服务干宕机。AOF支持秒级数据丢失（取决fsync策略，如果是everysec，最多丢失1秒的数据），仅仅是追加命令到AOF文件，操作轻量。
- RDB文件是以特定的二进制格式保存的，并且在Redis版本演进中有多个版本的RDB，所以存在老版本的Redis服务不兼容新版本的RDB格式的问题。
- AOF以一种易于理解和解析的格式包含所有操作的日志。你可以轻松地导出AOF文件进行分析，你也可以直接操作AOF文件来解决一些问题。比如，如果执行`FLUSHALL`命令意外地刷新了所有内容后，只要AOF文件没有被重写，删除最新命令并重启即可恢复之前的状态。

**综上：**

- Redis保存的数据丢失一些也没什么影响的话，可以选择使用RDB。
- 不建议单独使用AOF，因为时不时地创建一个RDB快照可以进行数据库备份、更快的重启以及解决AOF引擎错误。
- 如果保存的数据要求安全性比较高的话，建议同时开启RDB和AOF持久化或者开启RDB和AOF混合持久化。

### Redis内存管理

### Redis常见阻塞原因总结

### Redis性能优化

### Redis生产问题

#### 缓存穿透

#### 缓存击穿

#### 缓存雪崩
